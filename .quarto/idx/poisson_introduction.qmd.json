{"title":"Introduction to Poisson Modelling","markdown":{"yaml":{"title":"Introduction to Poisson Modelling","author":"Brittany Alexandra","date":"`r Sys.Date()`","format":{"html":{"theme":"cerulean","toc":true,"toc-depth":2,"toc-location":"left","code-fold":true,"code-tools":true}},"editor_options":{"markdown":{"wrap":72}}},"headingText":"I. Modelling processes that evolve randomly over time","containsRefs":false,"markdown":"\n\n\n------------------------------------------------------------------------\n\n```{r, include= FALSE, warning= FALSE, echo= FALSE}\nknitr::opts_chunk$set(warning = FALSE, message = FALSE)\n```\n\nWelcome to the soft introduction into Poisson modelling in R!\n\nMany things in life can be modeled with the poisson process as it quite\nliterally models the long-term average rate of which random events occur\nindependently. Its subjectively the most widely used model of a point\nprocess in time, and is suitable for modelling *frequencies* and\n*probabilities.*\n\n<br>\n\n[Examples of events that can be modeled by the poisson\ndistribution]{.underline}\n\n-   The frequency of goals scored by a team in a soccer match\n\n-   The probability two teams will score no goals in a soccer match\n\n-   The frequency of inbound calls to a call center\n\n-   The probability you receive 10 calls in one hour to a call center\n\n-   The frequency of unique visitors to a website\n\n-   The probability you receive 5 unique visitors to a website in 30\n    minutes\n\n-   The frequency walk-in arrivals to a physical location (customers or\n    patients)\n\n-   The probability you receive 30 walk-in arrivals to a physical\n    location in 2 hours.\n\n-   The frequency of volcanic eruptions\n\n-   The probability there will be 2 volcanic eruptions in the next 10\n    years.\n\n<br>\n\n[All of these scenarios have the following in common which allow us to\ndescribe the response:]{.underline}\n\n-   Response is discrete\n\n-   Response is right skewed (but approaches a normal distribution as mu\n    increases\n\n-   Bounded between 1 and infinity.\n\n-   Variance increases with the mean.\n\n<br>\n\n[When modelling with the any distribution in R, we will follow the\ngeneral format of]{.underline}\n\n1.  Choose a distribution for the response: Poisson\n\n2.  Choose a parameter to relate to explanatory terms: mu_i\n\n3.  Choose a link function: log\n\n4.   Choose explanatory terms (more on this later):\n\n5.  Additional parameters may be estimated/observed:\n\n<br>\n\n# II. Synthesis of the poisson model\n\n------------------------------------------------------------------------\n\n::: callout-note\n## ðŸ’¡ Key Takeaways:\n\n1.  **Poisson Log Link**: Ensures predicted values are +ve.\n\n2.  **Logistic Logit Link**: Logit is between -inf to inf. exp() ensures\n    predicted values are bound by 0 and 1.\n:::\n\nNow that we've introduced what kinds of real-world phenomena follow a\nPoisson distribution, from soccer goals to volcanic eruptions, you might\nbe wondering, \"How do we actually use this knowledge to build a\nstatistical model?\"\n\nIn classical linear regression, we assume the response has an\n**unbounded continuous range**. That works fine when you're predicting,\nsay, someone's income or weight. But in **Poisson regression**, we model\n**counts**... things like number of visitors, arrivals, or events,\nsituations where negative values make no sense.\n\nSo, instead of the standard form: $\\mu_i = \\beta_0 + \\beta_1 x_i$, we\napply a **log** **link function** to ensure our expected value, $\\mu_i$,\nis always **positive**. After all, a negative number of customer\nwalk-ins? That might get you fired ðŸ˜…\n\n<br>\n\n[**ðŸ”— Enter the Log Link**]{.underline}\n\nTo fix this, the Poisson GLM uses a \\*\\*log link function\\*\\*, which\ntransforms the expected value onto the real number line:\n\n$\\log(\\mu_i) = \\beta_0 + \\beta_1 x_i \\quad \\Rightarrow \\quad \\mu_i = \\exp(\\beta_0 + \\beta_1 x_i)$\n\n[This transformation has two key benefits:]{.underline}\n\n-   \\- $\\mu_i$ can never be negative\n\n-   \\- The model remains linear \\*\\*on the log scale\\*\\*\n\nAnd because of the log transformation, we interpret coefficients\n**multiplicatively**...\n\n<br>\n\nThe two plots below help visualize how the Poisson model transforms\nvalues using a **log link**. The log ensures the model stays linear on\nthe log scale, while the exponential ensures predictions for the mean\nresponse, Î¼, are always positive.\n\nThe **log link function** transforms expected counts $\\mu$ (which must\nbe â‰¥ 0) into linear predictors Î· on the real number line.\n\n```{r}\nlibrary(tidyverse)\n \n # Plot 1: Log Link Function Î· = log(Î¼)\nmu <- seq(0.01, 10, length.out = 500)\neta_from_mu <- log(mu)\ndf1 <- data.frame(mu = mu, eta = eta_from_mu)\n\n# Add a point to highlight transformation\nhighlight1 <- data.frame(mu = 5, eta = log(5))\n\nggplot(df1, aes(x = mu, y = eta)) +\n  geom_line(color = \"blue\", linewidth = 1) +\n  geom_point(data = highlight1, aes(x = mu, y = eta), color = \"red\", size = 3) +\n  geom_segment(\n    data = highlight1,\n    aes(x = mu, xend = mu, y = 0, yend = eta),\n    linetype = \"dashed\", color = \"gray\"\n  ) +\n  geom_segment(\n    data = highlight1,\n    aes(x = 0, xend = mu, y = eta, yend = eta),\n    linetype = \"dashed\", color = \"gray\"\n  ) +\n  geom_text(\n    data = highlight1,\n    aes(x = mu + 1.5, y = eta),\n    label = expression(paste(\"Î· = log(\", mu, \") = \", log(5))),\n    hjust = 0, size = 5\n  ) +\n  labs(\n    title = \"Log Link Function: Î· = log(Î¼)\",\n    x = expression(mu ~ \"(mean of response)\"),\n    y = expression(eta ~ \"(linear predictor)\")\n  ) +\n  theme_minimal(base_size = 14)\n```\n\nConversely, The **inverse link function** (exp) transforms any linear\npredictor Î· back into a valid, positive expected count.\n\n```{r}\n# Plot 2: Inverse link function (exp)\neta <- seq(-5, 5, length.out = 500)\nmu_from_eta <- exp(eta)\ndf2 <- data.frame(eta = eta, mu = mu_from_eta)\n\n# Highlight a point\nhighlight2 <- data.frame(eta = 1.6, mu = exp(1.6))\n\nggplot(df2, aes(x = eta, y = mu)) +\n  geom_line(color = \"forestgreen\", linewidth = 1) +\n  geom_point(data = highlight2, aes(x = eta, y = mu), color = \"red\", size = 3) +\n  geom_segment(\n    data = highlight2,\n    aes(x = eta, xend = eta, y = 0, yend = mu),\n    linetype = \"dashed\", color = \"gray\"\n  ) +\n  geom_segment(\n    data = highlight2,\n    aes(x = 0, xend = eta, y = mu, yend = mu),\n    linetype = \"dashed\", color = \"gray\"\n  ) +\n  geom_text(\n    data = highlight2,\n    aes(x = eta + 0.5, y = mu),\n    label = expression(paste(\"Î¼ = exp(\", eta, \") = \", round(exp(1.6), 1))),\n    hjust = 0, size = 5\n  ) +\n  labs(\n    title = \"Inverse Link Function: Î¼ = exp(Î·)\",\n    x = expression(eta ~ \"(linear predictor)\"),\n    y = expression(mu ~ \"(mean of response)\")\n  ) +\n  theme_minimal(base_size = 14)\n```\n\n# III. Explanatory Terms, and Interpretation\n\n------------------------------------------------------------------------\n\n::: callout-note\n## ðŸ’¡ Key Takeaways:\n\nHolding all over variables constant, for every n-unit increase in X, the\nexpected value of the response in multiplied by exp(B1)\n:::\n\n$\\log(\\mu) = \\beta_0 + \\beta_1 x$\n\n$\\mu = e^{\\beta_0 + \\beta_1x}$\n\n$=e^{\\beta_0} (e^{\\beta_1})^x$\n\n\\- When $x = 0$, the expected value equals $\\exp(\\beta_0)$.\n\n\\- For every **one-unit** increase in $x$, the expected value of the\nresponse is multiplied by $\\exp(\\beta_1)$.\n\n\\- For every **ten-unit** increase in $x$, the expected value of the\nresponse is multiplied by $\\exp(10 \\beta_1)$.\n\n\\- For every $n$**-unit** increase in $x$, the expected value of the\nresponse is multiplied by $\\exp(n \\beta_1)$.\n\n# IV. Parameter Estimation for Generalised Linear Models\n\n------------------------------------------------------------------------\n\n::: callout-note\n## ðŸ’¡ Key Takeaways:\n\nGLMs: Maximise the log-likelihood\n:::\n\n**Maximum Log-Likelihood**\n\nIn Generalised Linear Models (GLMs), we estimate the model\\'s\ncoefficients using a method called maximum likelihood estimation (MLE).\nThe core idea is this: for any given set of candidate parameters, we can\ncompute how likely it is that we would have observed our data, assuming\nthose parameters are true. This is the **likelihood**.\n\n-   If a data point lies close to the predicted curve (or line), it\n    contributes a high likelihood---it\\'s consistent with the model.\n\n-   If it\\'s far from the predicted values, it contributes a low\n    likelihood---it's inconsistent with the model.\n\n<br>\nMLE works by searching for the set of parameters (usually denoted as\n$\\beta$) that maximise the likelihood function. In other words, we\nchoose the parameters that make our observed data as likely as possible\nunder the model.\nSo, instead of minimising the residual sum of squares like in linear\nregression, we are maximising the log-likelihood in GLMs. This is\nparticularly useful when dealing with data where assumptions of constant\nvariance or normality don\\'t hold (e.g. count or binary data).\nA better-fitting model will have a higher log-likelihood, meaning it's\nmore consistent with the observed data.\n\n::: callout-note\nEven though two points might look equally close to the predicted line,\ntheir likelihood contributions can still differ due to how spread out\nthe underlying distribution is. In Poisson regression, this spread grows\nwith the mean, so deviations are penalized more harshly when the\nexpected value is small.\n:::\n\n```{r}\n\nset.seed(1)\nx <- 1:20\ny <- rpois(20, lambda = exp(0.2 * x))\nmodel <- glm(y ~ x, family = poisson())\npred <- predict(model, type = \"response\")\nlog_lik <- dpois(y, lambda = pred, log = TRUE)\n\ndf <- data.frame(x, y, pred, log_lik)\n\nggplot(df, aes(x, y)) +\n  geom_point(aes(color = log_lik), size = 5) +\n  geom_line(aes(y = pred), color = \"blue\", linetype = \"dashed\") +\n  scale_color_viridis_c(option = \"virdis\") +\n  labs(title = \"Log-Likelihood Contributions of Points\",\n       subtitle = \"Darker points contribute less to the log-likelihood\",\n       color = \"Log-Likelihood\") +\n  theme_minimal()\n \n```\n\n[To calculate the likelihood function...]{.underline}\n\n1.  Calculate the probability of observing the observations response\n\n2.  Take the product of these probabilities\n\n    $\\ell = \\log(L) = \\sum_{i=1}^{n} \\log \\left[ f(y_i; \\boldsymbol{\\beta}) \\right], \\ where \\ f(y_i; \\boldsymbol{\\beta}) \\ is \\ the \\ pmf \\ of \\ the \\ assumed \\ response \\ distribution$\n\n# V. Deviance as a Goodness of Fit\n\n------------------------------------------------------------------------\n\n::: callout-note\n## ðŸ’¡ Key Takeaways:\n\n1.  Parameter estimation is achieved by minimising the deviance\n\n2.  The more variance, the larger the deviance.\n\n3.  Increasing model complexity always decreases RSS (LR) / increases\n    MLE (GLM)\n\n4.  Ds â‰¤ D â‰¤ Dn\n\n5.  1 - pchisq(residuals, df) should be large if our model is correct.\n:::\n\nStatistical analysis does not simply end once we have fit a model. We\nmust determine whether or not our model seems appropriate.\n\"Goodness-of-fit\" is a property that describes how well the data appears\nto fit the model's assumptions. For a GLM, these assumptions are\n\n1.  The observations are independent.\n\n2.  $g (\\theta) = X \\beta$ after applying the link function, the\n    parameter of interest is a linear combination of the explanatory\n    terms.\n\n3.  Each response comes from the assumed distribution.\n\n<br>\n\nWhen considering a simple GLM, we can add all sorts of explanatory\nterms;\n\n-   Additional numeric explanatory variables\n\n-   Factors via dummy variables\n\n-   Non linear relationships\n\n-   All of the above\n\n<br>\n\nBut, adding explanatory terms is sometimes said to make it more\n\"complex\", or increase its \"complexity.\" In consequence, increasing a\nmodels complexity will almost always\n\n-   Decrease the residual sum of squares (linear regression), OR\n\n-   Increase the maximised log-likelihood (GLMS).\n\n**Model Saturation**\n\n```{r}\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(splines)\n\n# Simulate more nonlinear data\nset.seed(42)\nx <- seq(0, 10, length.out = 10)\ny <- 0.5 * x^2 - 3 * x + 5 + rnorm(10, mean = 0, sd = 2)\ndf <- data.frame(x = x, y = y)\n\n# 1. Underfit model: simple linear regression\nmodel_lm <- lm(y ~ x, data = df)\ndf$y_lm <- predict(model_lm)\n\n# 2. Moderately saturated: smooth spline with lower spar (more flexible)\nspline_fit <- smooth.spline(df$x, df$y, spar = 0.4)  # lower spar for more \"wiggle\"\nx_dense <- seq(min(x), max(x), length.out = 200)\nspline_pred <- predict(spline_fit, x_dense)\ndf_spline <- data.frame(x = spline_pred$x, y = spline_pred$y)\n\n# 3. Fully saturated: perfect interpolation\ndf$y_sat <- df$y  # fitted values equal observed values\n\n# Plot 1: Underfit (Linear Regression)\np1 <- ggplot(df, aes(x, y)) +\n  geom_point(color = \"blue\") +\n  geom_line(aes(y = y_lm), color = \"black\") +\n  geom_segment(aes(xend = x, yend = y_lm), color = \"red\", linetype = \"dashed\") +\n  ggtitle(\"Underfit Model\") +\n  theme_minimal()\n\n# Plot 2: Moderately Saturated (Spline Fit)\np2 <- ggplot(df, aes(x, y)) +\n  geom_point(color = \"blue\") +\n  geom_line(data = df_spline, aes(x = x, y = y), color = \"orange\") +\n  geom_segment(aes(xend = x, yend = predict(spline_fit, x)$y), color = \"red\", linetype = \"dashed\") +\n  ggtitle(\"Moderately Saturated Model\") +\n  theme_minimal()\n\n# Plot 3: Fully Saturated (Perfect Fit)\np3 <- ggplot(df, aes(x, y)) +\n  geom_point(color = \"blue\") +\n  geom_line(aes(y = y_sat), color = \"green\") +\n  ggtitle(\"Fully Saturated Model\") +\n  theme_minimal()\n\n# Combine the three plots\ngrid.arrange(p1, p2, p3, nrow = 1)\n\n\n```\n\nIn almost all cases, the saturated model is not a sensible model as\n\n1.  If we were to take another sample from the population, their\n    observations would not exhibit the exact same trend.\n\n2.  The exact pattern we see therefore reflects the random white noise\n    inherent in our particular sample, and not the underlying\n    relationship that exists in the population.\n\n3.  We should try avoid over fitting models, such as the saturated\n    models, so that they only describe the underlying trends in the\n    population, and not random noise from our sample\n","srcMarkdownNoYaml":"\n\n# I. Modelling processes that evolve randomly over time\n\n------------------------------------------------------------------------\n\n```{r, include= FALSE, warning= FALSE, echo= FALSE}\nknitr::opts_chunk$set(warning = FALSE, message = FALSE)\n```\n\nWelcome to the soft introduction into Poisson modelling in R!\n\nMany things in life can be modeled with the poisson process as it quite\nliterally models the long-term average rate of which random events occur\nindependently. Its subjectively the most widely used model of a point\nprocess in time, and is suitable for modelling *frequencies* and\n*probabilities.*\n\n<br>\n\n[Examples of events that can be modeled by the poisson\ndistribution]{.underline}\n\n-   The frequency of goals scored by a team in a soccer match\n\n-   The probability two teams will score no goals in a soccer match\n\n-   The frequency of inbound calls to a call center\n\n-   The probability you receive 10 calls in one hour to a call center\n\n-   The frequency of unique visitors to a website\n\n-   The probability you receive 5 unique visitors to a website in 30\n    minutes\n\n-   The frequency walk-in arrivals to a physical location (customers or\n    patients)\n\n-   The probability you receive 30 walk-in arrivals to a physical\n    location in 2 hours.\n\n-   The frequency of volcanic eruptions\n\n-   The probability there will be 2 volcanic eruptions in the next 10\n    years.\n\n<br>\n\n[All of these scenarios have the following in common which allow us to\ndescribe the response:]{.underline}\n\n-   Response is discrete\n\n-   Response is right skewed (but approaches a normal distribution as mu\n    increases\n\n-   Bounded between 1 and infinity.\n\n-   Variance increases with the mean.\n\n<br>\n\n[When modelling with the any distribution in R, we will follow the\ngeneral format of]{.underline}\n\n1.  Choose a distribution for the response: Poisson\n\n2.  Choose a parameter to relate to explanatory terms: mu_i\n\n3.  Choose a link function: log\n\n4.   Choose explanatory terms (more on this later):\n\n5.  Additional parameters may be estimated/observed:\n\n<br>\n\n# II. Synthesis of the poisson model\n\n------------------------------------------------------------------------\n\n::: callout-note\n## ðŸ’¡ Key Takeaways:\n\n1.  **Poisson Log Link**: Ensures predicted values are +ve.\n\n2.  **Logistic Logit Link**: Logit is between -inf to inf. exp() ensures\n    predicted values are bound by 0 and 1.\n:::\n\nNow that we've introduced what kinds of real-world phenomena follow a\nPoisson distribution, from soccer goals to volcanic eruptions, you might\nbe wondering, \"How do we actually use this knowledge to build a\nstatistical model?\"\n\nIn classical linear regression, we assume the response has an\n**unbounded continuous range**. That works fine when you're predicting,\nsay, someone's income or weight. But in **Poisson regression**, we model\n**counts**... things like number of visitors, arrivals, or events,\nsituations where negative values make no sense.\n\nSo, instead of the standard form: $\\mu_i = \\beta_0 + \\beta_1 x_i$, we\napply a **log** **link function** to ensure our expected value, $\\mu_i$,\nis always **positive**. After all, a negative number of customer\nwalk-ins? That might get you fired ðŸ˜…\n\n<br>\n\n[**ðŸ”— Enter the Log Link**]{.underline}\n\nTo fix this, the Poisson GLM uses a \\*\\*log link function\\*\\*, which\ntransforms the expected value onto the real number line:\n\n$\\log(\\mu_i) = \\beta_0 + \\beta_1 x_i \\quad \\Rightarrow \\quad \\mu_i = \\exp(\\beta_0 + \\beta_1 x_i)$\n\n[This transformation has two key benefits:]{.underline}\n\n-   \\- $\\mu_i$ can never be negative\n\n-   \\- The model remains linear \\*\\*on the log scale\\*\\*\n\nAnd because of the log transformation, we interpret coefficients\n**multiplicatively**...\n\n<br>\n\nThe two plots below help visualize how the Poisson model transforms\nvalues using a **log link**. The log ensures the model stays linear on\nthe log scale, while the exponential ensures predictions for the mean\nresponse, Î¼, are always positive.\n\nThe **log link function** transforms expected counts $\\mu$ (which must\nbe â‰¥ 0) into linear predictors Î· on the real number line.\n\n```{r}\nlibrary(tidyverse)\n \n # Plot 1: Log Link Function Î· = log(Î¼)\nmu <- seq(0.01, 10, length.out = 500)\neta_from_mu <- log(mu)\ndf1 <- data.frame(mu = mu, eta = eta_from_mu)\n\n# Add a point to highlight transformation\nhighlight1 <- data.frame(mu = 5, eta = log(5))\n\nggplot(df1, aes(x = mu, y = eta)) +\n  geom_line(color = \"blue\", linewidth = 1) +\n  geom_point(data = highlight1, aes(x = mu, y = eta), color = \"red\", size = 3) +\n  geom_segment(\n    data = highlight1,\n    aes(x = mu, xend = mu, y = 0, yend = eta),\n    linetype = \"dashed\", color = \"gray\"\n  ) +\n  geom_segment(\n    data = highlight1,\n    aes(x = 0, xend = mu, y = eta, yend = eta),\n    linetype = \"dashed\", color = \"gray\"\n  ) +\n  geom_text(\n    data = highlight1,\n    aes(x = mu + 1.5, y = eta),\n    label = expression(paste(\"Î· = log(\", mu, \") = \", log(5))),\n    hjust = 0, size = 5\n  ) +\n  labs(\n    title = \"Log Link Function: Î· = log(Î¼)\",\n    x = expression(mu ~ \"(mean of response)\"),\n    y = expression(eta ~ \"(linear predictor)\")\n  ) +\n  theme_minimal(base_size = 14)\n```\n\nConversely, The **inverse link function** (exp) transforms any linear\npredictor Î· back into a valid, positive expected count.\n\n```{r}\n# Plot 2: Inverse link function (exp)\neta <- seq(-5, 5, length.out = 500)\nmu_from_eta <- exp(eta)\ndf2 <- data.frame(eta = eta, mu = mu_from_eta)\n\n# Highlight a point\nhighlight2 <- data.frame(eta = 1.6, mu = exp(1.6))\n\nggplot(df2, aes(x = eta, y = mu)) +\n  geom_line(color = \"forestgreen\", linewidth = 1) +\n  geom_point(data = highlight2, aes(x = eta, y = mu), color = \"red\", size = 3) +\n  geom_segment(\n    data = highlight2,\n    aes(x = eta, xend = eta, y = 0, yend = mu),\n    linetype = \"dashed\", color = \"gray\"\n  ) +\n  geom_segment(\n    data = highlight2,\n    aes(x = 0, xend = eta, y = mu, yend = mu),\n    linetype = \"dashed\", color = \"gray\"\n  ) +\n  geom_text(\n    data = highlight2,\n    aes(x = eta + 0.5, y = mu),\n    label = expression(paste(\"Î¼ = exp(\", eta, \") = \", round(exp(1.6), 1))),\n    hjust = 0, size = 5\n  ) +\n  labs(\n    title = \"Inverse Link Function: Î¼ = exp(Î·)\",\n    x = expression(eta ~ \"(linear predictor)\"),\n    y = expression(mu ~ \"(mean of response)\")\n  ) +\n  theme_minimal(base_size = 14)\n```\n\n# III. Explanatory Terms, and Interpretation\n\n------------------------------------------------------------------------\n\n::: callout-note\n## ðŸ’¡ Key Takeaways:\n\nHolding all over variables constant, for every n-unit increase in X, the\nexpected value of the response in multiplied by exp(B1)\n:::\n\n$\\log(\\mu) = \\beta_0 + \\beta_1 x$\n\n$\\mu = e^{\\beta_0 + \\beta_1x}$\n\n$=e^{\\beta_0} (e^{\\beta_1})^x$\n\n\\- When $x = 0$, the expected value equals $\\exp(\\beta_0)$.\n\n\\- For every **one-unit** increase in $x$, the expected value of the\nresponse is multiplied by $\\exp(\\beta_1)$.\n\n\\- For every **ten-unit** increase in $x$, the expected value of the\nresponse is multiplied by $\\exp(10 \\beta_1)$.\n\n\\- For every $n$**-unit** increase in $x$, the expected value of the\nresponse is multiplied by $\\exp(n \\beta_1)$.\n\n# IV. Parameter Estimation for Generalised Linear Models\n\n------------------------------------------------------------------------\n\n::: callout-note\n## ðŸ’¡ Key Takeaways:\n\nGLMs: Maximise the log-likelihood\n:::\n\n**Maximum Log-Likelihood**\n\nIn Generalised Linear Models (GLMs), we estimate the model\\'s\ncoefficients using a method called maximum likelihood estimation (MLE).\nThe core idea is this: for any given set of candidate parameters, we can\ncompute how likely it is that we would have observed our data, assuming\nthose parameters are true. This is the **likelihood**.\n\n-   If a data point lies close to the predicted curve (or line), it\n    contributes a high likelihood---it\\'s consistent with the model.\n\n-   If it\\'s far from the predicted values, it contributes a low\n    likelihood---it's inconsistent with the model.\n\n<br>\nMLE works by searching for the set of parameters (usually denoted as\n$\\beta$) that maximise the likelihood function. In other words, we\nchoose the parameters that make our observed data as likely as possible\nunder the model.\nSo, instead of minimising the residual sum of squares like in linear\nregression, we are maximising the log-likelihood in GLMs. This is\nparticularly useful when dealing with data where assumptions of constant\nvariance or normality don\\'t hold (e.g. count or binary data).\nA better-fitting model will have a higher log-likelihood, meaning it's\nmore consistent with the observed data.\n\n::: callout-note\nEven though two points might look equally close to the predicted line,\ntheir likelihood contributions can still differ due to how spread out\nthe underlying distribution is. In Poisson regression, this spread grows\nwith the mean, so deviations are penalized more harshly when the\nexpected value is small.\n:::\n\n```{r}\n\nset.seed(1)\nx <- 1:20\ny <- rpois(20, lambda = exp(0.2 * x))\nmodel <- glm(y ~ x, family = poisson())\npred <- predict(model, type = \"response\")\nlog_lik <- dpois(y, lambda = pred, log = TRUE)\n\ndf <- data.frame(x, y, pred, log_lik)\n\nggplot(df, aes(x, y)) +\n  geom_point(aes(color = log_lik), size = 5) +\n  geom_line(aes(y = pred), color = \"blue\", linetype = \"dashed\") +\n  scale_color_viridis_c(option = \"virdis\") +\n  labs(title = \"Log-Likelihood Contributions of Points\",\n       subtitle = \"Darker points contribute less to the log-likelihood\",\n       color = \"Log-Likelihood\") +\n  theme_minimal()\n \n```\n\n[To calculate the likelihood function...]{.underline}\n\n1.  Calculate the probability of observing the observations response\n\n2.  Take the product of these probabilities\n\n    $\\ell = \\log(L) = \\sum_{i=1}^{n} \\log \\left[ f(y_i; \\boldsymbol{\\beta}) \\right], \\ where \\ f(y_i; \\boldsymbol{\\beta}) \\ is \\ the \\ pmf \\ of \\ the \\ assumed \\ response \\ distribution$\n\n# V. Deviance as a Goodness of Fit\n\n------------------------------------------------------------------------\n\n::: callout-note\n## ðŸ’¡ Key Takeaways:\n\n1.  Parameter estimation is achieved by minimising the deviance\n\n2.  The more variance, the larger the deviance.\n\n3.  Increasing model complexity always decreases RSS (LR) / increases\n    MLE (GLM)\n\n4.  Ds â‰¤ D â‰¤ Dn\n\n5.  1 - pchisq(residuals, df) should be large if our model is correct.\n:::\n\nStatistical analysis does not simply end once we have fit a model. We\nmust determine whether or not our model seems appropriate.\n\"Goodness-of-fit\" is a property that describes how well the data appears\nto fit the model's assumptions. For a GLM, these assumptions are\n\n1.  The observations are independent.\n\n2.  $g (\\theta) = X \\beta$ after applying the link function, the\n    parameter of interest is a linear combination of the explanatory\n    terms.\n\n3.  Each response comes from the assumed distribution.\n\n<br>\n\nWhen considering a simple GLM, we can add all sorts of explanatory\nterms;\n\n-   Additional numeric explanatory variables\n\n-   Factors via dummy variables\n\n-   Non linear relationships\n\n-   All of the above\n\n<br>\n\nBut, adding explanatory terms is sometimes said to make it more\n\"complex\", or increase its \"complexity.\" In consequence, increasing a\nmodels complexity will almost always\n\n-   Decrease the residual sum of squares (linear regression), OR\n\n-   Increase the maximised log-likelihood (GLMS).\n\n**Model Saturation**\n\n```{r}\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(splines)\n\n# Simulate more nonlinear data\nset.seed(42)\nx <- seq(0, 10, length.out = 10)\ny <- 0.5 * x^2 - 3 * x + 5 + rnorm(10, mean = 0, sd = 2)\ndf <- data.frame(x = x, y = y)\n\n# 1. Underfit model: simple linear regression\nmodel_lm <- lm(y ~ x, data = df)\ndf$y_lm <- predict(model_lm)\n\n# 2. Moderately saturated: smooth spline with lower spar (more flexible)\nspline_fit <- smooth.spline(df$x, df$y, spar = 0.4)  # lower spar for more \"wiggle\"\nx_dense <- seq(min(x), max(x), length.out = 200)\nspline_pred <- predict(spline_fit, x_dense)\ndf_spline <- data.frame(x = spline_pred$x, y = spline_pred$y)\n\n# 3. Fully saturated: perfect interpolation\ndf$y_sat <- df$y  # fitted values equal observed values\n\n# Plot 1: Underfit (Linear Regression)\np1 <- ggplot(df, aes(x, y)) +\n  geom_point(color = \"blue\") +\n  geom_line(aes(y = y_lm), color = \"black\") +\n  geom_segment(aes(xend = x, yend = y_lm), color = \"red\", linetype = \"dashed\") +\n  ggtitle(\"Underfit Model\") +\n  theme_minimal()\n\n# Plot 2: Moderately Saturated (Spline Fit)\np2 <- ggplot(df, aes(x, y)) +\n  geom_point(color = \"blue\") +\n  geom_line(data = df_spline, aes(x = x, y = y), color = \"orange\") +\n  geom_segment(aes(xend = x, yend = predict(spline_fit, x)$y), color = \"red\", linetype = \"dashed\") +\n  ggtitle(\"Moderately Saturated Model\") +\n  theme_minimal()\n\n# Plot 3: Fully Saturated (Perfect Fit)\np3 <- ggplot(df, aes(x, y)) +\n  geom_point(color = \"blue\") +\n  geom_line(aes(y = y_sat), color = \"green\") +\n  ggtitle(\"Fully Saturated Model\") +\n  theme_minimal()\n\n# Combine the three plots\ngrid.arrange(p1, p2, p3, nrow = 1)\n\n\n```\n\nIn almost all cases, the saturated model is not a sensible model as\n\n1.  If we were to take another sample from the population, their\n    observations would not exhibit the exact same trend.\n\n2.  The exact pattern we see therefore reflects the random white noise\n    inherent in our particular sample, and not the underlying\n    relationship that exists in the population.\n\n3.  We should try avoid over fitting models, such as the saturated\n    models, so that they only describe the underlying trends in the\n    population, and not random noise from our sample\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"toc-depth":2,"output-file":"poisson_introduction.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","theme":"cosmo","title":"Introduction to Poisson Modelling","author":"Brittany Alexandra","date":"`r Sys.Date()`","editor_options":{"markdown":{"wrap":72}},"toc-location":"left"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}