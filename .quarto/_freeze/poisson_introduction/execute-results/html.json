{
  "hash": "740ba1fc9f45385c17275bd2ec5968b1",
  "result": {
    "markdown": "---\ntitle: \"Introduction to Poisson Modelling\"\nauthor: \"Brittany Alexandra\"\ndate: \"2025-06-04\"\n\nformat:\n  html:\n    theme: cerulean\n    toc: true\n    toc-depth: 2          \n    toc-location: left    \n    code-fold: true\n    code-tools: true\n\neditor_options: \n  markdown: \n    wrap: 72\n---\n\n\n# I. Modelling processes that evolve randomly over time\n\n------------------------------------------------------------------------\n\n\n\n\n\nWelcome to the soft introduction into Poisson modelling in R!\n\nMany things in life can be modeled with the poisson process as it quite\nliterally models the long-term average rate of which random events occur\nindependently. Its subjectively the most widely used model of a point\nprocess in time, and is suitable for modelling *frequencies* and\nprobabilities*.*\n\n<br>\n\n[Examples of events that can be modeled by the poisson\ndistribution]{.underline}\n\n-   The frequency of goals scored by a team in a soccer match\n\n-   The probability two teams will score no goals in a soccer match\n\n-   The frequency of inbound calls to a call center\n\n-   The probability you receive 10 calls in one hour to a call center\n\n-   The frequency of unique visitors to a website\n\n-   The probability you receive 5 unique visitors to a website in 30\n    minutes\n\n-   The frequency walk-in arrivals to a physical location (customers or\n    patients)\n\n-   The probability you receive 30 walk-in arrivals to a physical\n    location in 2 hours.\n\n-   The frequency of volcanic eruptions\n\n-   The probability there will be 2 volcanic eruptions in the next 10\n    years.\n\n<br>\n\n[All of these scenarios have the following in common which allow us to\ndescribe the response:]{.underline}\n\n-   Response is discrete\n\n-   Response is right skewed (but approaches a normal distribution as mu\n    increases\n\n-   Bounded between 1 and infinity.\n\n-   Variance increases with the mean.\n\n<br>\n\n# II. Synthesis of the poisson model\n\n------------------------------------------------------------------------\n\n::: callout-note\n## ðŸ’¡ Key Takeaways:\n\n1.  **Poisson Log Link**: Ensures predicted values are +ve.\n\n2.  **Logistic Logit Link**: Logit is between -inf to inf. exp() ensures\n    predicted values are bound by 0 and 1.\n:::\n\nNow that we've introduced what kinds of real-world phenomena follow a\nPoisson distribution, from soccer goals to volcanic eruptions, you might\nbe wondering, \"How do we actually use this knowledge to build a\nstatistical model?\"\n\nIn classical linear regression, we assume the response has an\n**unbounded continuous range**. That works fine when you're predicting,\nsay, someone's income or weight. But in **Poisson regression**, we model\n**counts**... things like number of visitors, arrivals, or events,\nsituations where negative values make no sense.\n\nSo, instead of the standard form: $\\mu_i = \\beta_0 + \\beta_1 x_i$, we\napply a **log** **link function** to ensure our expected value, $\\mu_i$,\nis always **positive**. After all, a negative number of customer\nwalk-ins? That might get you fired ðŸ˜…\n\n<br>\n\n[**ðŸ”— Enter the Log Link**]{.underline}\n\nTo fix this, the Poisson GLM uses a \\*\\*log link function\\*\\*, which\ntransforms the expected value onto the real number line:\n\n$\\log(\\mu_i) = \\beta_0 + \\beta_1 x_i \\quad \\Rightarrow \\quad \\mu_i = \\exp(\\beta_0 + \\beta_1 x_i)$\n\n[This transformation has two key benefits:]{.underline}\n\n-   \\- $\\mu_i$ can never be negative\n\n-   \\- The model remains linear \\*\\*on the log scale\\*\\*\n\nAnd because of the log transformation, we interpret coefficients\n**multiplicatively**...\n\n<br>\n\nThe two plots below help visualize how the Poisson model transforms\nvalues using a **log link**. The log ensures the model stays linear on\nthe log scale, while the exponential ensures predictions for the mean\nresponse, Î¼, are always positive.\n\nThe **log link function** transforms expected counts $\\mu$ (which must\nbe â‰¥ 0) into linear predictors Î· on the real number line.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n \n # Plot 1: Log Link Function Î· = log(Î¼)\nmu <- seq(0.01, 10, length.out = 500)\neta_from_mu <- log(mu)\ndf1 <- data.frame(mu = mu, eta = eta_from_mu)\n\n# Add a point to highlight transformation\nhighlight1 <- data.frame(mu = 5, eta = log(5))\n\nggplot(df1, aes(x = mu, y = eta)) +\n  geom_line(color = \"blue\", linewidth = 1) +\n  geom_point(data = highlight1, aes(x = mu, y = eta), color = \"red\", size = 3) +\n  geom_segment(\n    data = highlight1,\n    aes(x = mu, xend = mu, y = 0, yend = eta),\n    linetype = \"dashed\", color = \"gray\"\n  ) +\n  geom_segment(\n    data = highlight1,\n    aes(x = 0, xend = mu, y = eta, yend = eta),\n    linetype = \"dashed\", color = \"gray\"\n  ) +\n  geom_text(\n    data = highlight1,\n    aes(x = mu + 1.5, y = eta),\n    label = expression(paste(\"Î· = log(\", mu, \") = \", log(5))),\n    hjust = 0, size = 5\n  ) +\n  labs(\n    title = \"Log Link Function: Î· = log(Î¼)\",\n    x = expression(mu ~ \"(mean of response)\"),\n    y = expression(eta ~ \"(linear predictor)\")\n  ) +\n  theme_minimal(base_size = 14)\n```\n\n::: {.cell-output-display}\n![](poisson_introduction_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nConversely, The **inverse link function** (exp) transforms any linear\npredictor Î· back into a valid, positive expected count.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot 2: Inverse link function (exp)\neta <- seq(-5, 5, length.out = 500)\nmu_from_eta <- exp(eta)\ndf2 <- data.frame(eta = eta, mu = mu_from_eta)\n\n# Highlight a point\nhighlight2 <- data.frame(eta = 1.6, mu = exp(1.6))\n\nggplot(df2, aes(x = eta, y = mu)) +\n  geom_line(color = \"forestgreen\", linewidth = 1) +\n  geom_point(data = highlight2, aes(x = eta, y = mu), color = \"red\", size = 3) +\n  geom_segment(\n    data = highlight2,\n    aes(x = eta, xend = eta, y = 0, yend = mu),\n    linetype = \"dashed\", color = \"gray\"\n  ) +\n  geom_segment(\n    data = highlight2,\n    aes(x = 0, xend = eta, y = mu, yend = mu),\n    linetype = \"dashed\", color = \"gray\"\n  ) +\n  geom_text(\n    data = highlight2,\n    aes(x = eta + 0.5, y = mu),\n    label = expression(paste(\"Î¼ = exp(\", eta, \") = \", round(exp(1.6), 1))),\n    hjust = 0, size = 5\n  ) +\n  labs(\n    title = \"Inverse Link Function: Î¼ = exp(Î·)\",\n    x = expression(eta ~ \"(linear predictor)\"),\n    y = expression(mu ~ \"(mean of response)\")\n  ) +\n  theme_minimal(base_size = 14)\n```\n\n::: {.cell-output-display}\n![](poisson_introduction_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n# III. Explanatory Terms, and Interpretation\n\n------------------------------------------------------------------------\n\n::: callout-note\n## ðŸ’¡ Key Takeaways:\n\nHolding all over variables constant, for every n-unit increase in X, the\nexpected value of the response in multiplied by exp(nB1)\n:::\n\n$\\log(\\mu) = \\beta_0 + \\beta_1 x$\n\n$\\mu = e^{\\beta_0 + \\beta_1x}$\n\n$=e^{\\beta_0} (e^{\\beta_1})^x$\n\n\\- When $x = 0$, the expected value equals $\\exp(\\beta_0)$.\n\n\\- For every **one-unit** increase in $x$, the expected value of the\nresponse is multiplied by $\\exp(\\beta_1)$.\n\n\\- For every **ten-unit** increase in $x$, the expected value of the\nresponse is multiplied by $\\exp(10 \\beta_1)$.\n\n\\- For every $n$**-unit** increase in $x$, the expected value of the\nresponse is multiplied by $\\exp(n \\beta_1)$.\n\n# IV. Parameter Estimation for Generalised Linear Models\n\n------------------------------------------------------------------------\n\n::: callout-note\n## ðŸ’¡ Key Takeaways:\n\nGLMs: Maximise the log-likelihood\n:::\n\n**Maximum Log-Likelihood**\n\nAs the LSE does not accommodate for increases in variance (and therefore\nneeds to adjust weighting for the residuals), we use the maximum\nlikelihood function. We aim to choose coefficients that provide the\nhighest possible log-likelihood.\n\n[To calculate the likelihood function...]{.underline}\n\n1.  Calculate the probability of observing the observations response\n\n2.  Take the product of these probabilities\n\n    $\\ell = \\log(L) = \\sum_{i=1}^{n} \\log \\left[ f(y_i; \\boldsymbol{\\beta}) \\right], \\ where \\ f(y_i; \\boldsymbol{\\beta}) \\ is \\ the \\ pmf \\ of \\ the \\ assumed \\ response \\ distribution$\n\n# V. Deviance as a Goodness of Fit\n\n------------------------------------------------------------------------\n\n::: callout-note\n## ðŸ’¡ Key Takeaways:\n\n1.  Parameter estimation is achieved by minimising the deviance\n\n2.  The more variance, the larger the deviance.\n\n3.  Increasing model complexity always decreases RSS (LR) / increases\n    MLE (GLM)\n\n4.  Ds â‰¤ D â‰¤ Dn\n\n5.  1 - pchisq(residuals, df) should be large if our model is correct.\n:::\n",
    "supporting": [
      "poisson_introduction_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}