[
  {
    "objectID": "poisson_introduction.html",
    "href": "poisson_introduction.html",
    "title": "Introduction to Poisson Modelling",
    "section": "",
    "text": "I. Modelling processes that evolve randomly over time\n\nWelcome to the soft introduction into Poisson modelling in R!\nMany things in life can be modeled with the poisson process as it quite literally models the long-term average rate of which random events occur independently. Its subjectively the most widely used model of a point process in time, and is suitable for modelling frequencies and probabilities.\n\nExamples of events that can be modeled by the poisson distribution\n\nThe frequency of goals scored by a team in a soccer match\nThe probability two teams will score no goals in a soccer match\nThe frequency of inbound calls to a call center\nThe probability you receive 10 calls in one hour to a call center\nThe frequency of unique visitors to a website\nThe probability you receive 5 unique visitors to a website in 30 minutes\nThe frequency walk-in arrivals to a physical location (customers or patients)\nThe probability you receive 30 walk-in arrivals to a physical location in 2 hours.\nThe frequency of volcanic eruptions\nThe probability there will be 2 volcanic eruptions in the next 10 years.\n\n\nAll of these scenarios have the following in common which allow us to describe the response:\n\nResponse is discrete\nResponse is right skewed (but approaches a normal distribution as mu increases\nBounded between 1 and infinity.\nVariance increases with the mean.\n\n\nWhen modelling with the any distribution in R, we will follow the general format of\n\nChoose a distribution for the response: Poisson\nChoose a parameter to relate to explanatory terms: mu_i\nChoose a link function: log\nChoose explanatory terms (more on this later):\nAdditional parameters may be estimated/observed:\n\n\n\n\nII. Synthesis of the poisson model\n\n\n\n\n\n\n\nüí° Key Takeaways:\n\n\n\n\nPoisson Log Link: Ensures predicted values are +ve.\nLogistic Logit Link: Logit is between -inf to inf. exp() ensures predicted values are bound by 0 and 1.\n\n\n\nNow that we‚Äôve introduced what kinds of real-world phenomena follow a Poisson distribution, from soccer goals to volcanic eruptions, you might be wondering, ‚ÄúHow do we actually use this knowledge to build a statistical model?‚Äù\nIn classical linear regression, we assume the response has an unbounded continuous range. That works fine when you‚Äôre predicting, say, someone‚Äôs income or weight. But in Poisson regression, we model counts‚Ä¶ things like number of visitors, arrivals, or events, situations where negative values make no sense.\nSo, instead of the standard form: \\(\\mu_i = \\beta_0 + \\beta_1 x_i\\), we apply a log link function to ensure our expected value, \\(\\mu_i\\), is always positive. After all, a negative number of customer walk-ins? That might get you fired üòÖ\n\nüîó Enter the Log Link\nTo fix this, the Poisson GLM uses a **log link function**, which transforms the expected value onto the real number line:\n\\(\\log(\\mu_i) = \\beta_0 + \\beta_1 x_i \\quad \\Rightarrow \\quad \\mu_i = \\exp(\\beta_0 + \\beta_1 x_i)\\)\nThis transformation has two key benefits:\n\n- \\(\\mu_i\\) can never be negative\n- The model remains linear **on the log scale**\n\nAnd because of the log transformation, we interpret coefficients multiplicatively‚Ä¶\n\nThe two plots below help visualize how the Poisson model transforms values using a log link. The log ensures the model stays linear on the log scale, while the exponential ensures predictions for the mean response, Œº, are always positive.\nThe log link function transforms expected counts \\(\\mu\\) (which must be ‚â• 0) into linear predictors Œ∑ on the real number line.\n\n\nCode\nlibrary(tidyverse)\n \n # Plot 1: Log Link Function Œ∑ = log(Œº)\nmu &lt;- seq(0.01, 10, length.out = 500)\neta_from_mu &lt;- log(mu)\ndf1 &lt;- data.frame(mu = mu, eta = eta_from_mu)\n\n# Add a point to highlight transformation\nhighlight1 &lt;- data.frame(mu = 5, eta = log(5))\n\nggplot(df1, aes(x = mu, y = eta)) +\n  geom_line(color = \"blue\", linewidth = 1) +\n  geom_point(data = highlight1, aes(x = mu, y = eta), color = \"red\", size = 3) +\n  geom_segment(\n    data = highlight1,\n    aes(x = mu, xend = mu, y = 0, yend = eta),\n    linetype = \"dashed\", color = \"gray\"\n  ) +\n  geom_segment(\n    data = highlight1,\n    aes(x = 0, xend = mu, y = eta, yend = eta),\n    linetype = \"dashed\", color = \"gray\"\n  ) +\n  geom_text(\n    data = highlight1,\n    aes(x = mu + 1.5, y = eta),\n    label = expression(paste(\"Œ∑ = log(\", mu, \") = \", log(5))),\n    hjust = 0, size = 5\n  ) +\n  labs(\n    title = \"Log Link Function: Œ∑ = log(Œº)\",\n    x = expression(mu ~ \"(mean of response)\"),\n    y = expression(eta ~ \"(linear predictor)\")\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\nConversely, The inverse link function (exp) transforms any linear predictor Œ∑ back into a valid, positive expected count.\n\n\nCode\n# Plot 2: Inverse link function (exp)\neta &lt;- seq(-5, 5, length.out = 500)\nmu_from_eta &lt;- exp(eta)\ndf2 &lt;- data.frame(eta = eta, mu = mu_from_eta)\n\n# Highlight a point\nhighlight2 &lt;- data.frame(eta = 1.6, mu = exp(1.6))\n\nggplot(df2, aes(x = eta, y = mu)) +\n  geom_line(color = \"forestgreen\", linewidth = 1) +\n  geom_point(data = highlight2, aes(x = eta, y = mu), color = \"red\", size = 3) +\n  geom_segment(\n    data = highlight2,\n    aes(x = eta, xend = eta, y = 0, yend = mu),\n    linetype = \"dashed\", color = \"gray\"\n  ) +\n  geom_segment(\n    data = highlight2,\n    aes(x = 0, xend = eta, y = mu, yend = mu),\n    linetype = \"dashed\", color = \"gray\"\n  ) +\n  geom_text(\n    data = highlight2,\n    aes(x = eta + 0.5, y = mu),\n    label = expression(paste(\"Œº = exp(\", eta, \") = \", round(exp(1.6), 1))),\n    hjust = 0, size = 5\n  ) +\n  labs(\n    title = \"Inverse Link Function: Œº = exp(Œ∑)\",\n    x = expression(eta ~ \"(linear predictor)\"),\n    y = expression(mu ~ \"(mean of response)\")\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\nIII. Explanatory Terms, and Interpretation\n\n\n\n\n\n\n\nüí° Key Takeaways:\n\n\n\nHolding all over variables constant, for every n-unit increase in X, the expected value of the response in multiplied by exp(B1)\n\n\n\\(\\log(\\mu) = \\beta_0 + \\beta_1 x\\)\n\\(\\mu = e^{\\beta_0 + \\beta_1x}\\)\n\\(=e^{\\beta_0} (e^{\\beta_1})^x\\)\n- When \\(x = 0\\), the expected value equals \\(\\exp(\\beta_0)\\).\n- For every one-unit increase in \\(x\\), the expected value of the response is multiplied by \\(\\exp(\\beta_1)\\).\n- For every ten-unit increase in \\(x\\), the expected value of the response is multiplied by \\(\\exp(10 \\beta_1)\\).\n- For every \\(n\\)-unit increase in \\(x\\), the expected value of the response is multiplied by \\(\\exp(n \\beta_1)\\).\n\n\nIV. Parameter Estimation for Generalised Linear Models\n\n\n\n\n\n\n\nüí° Key Takeaways:\n\n\n\nGLMs: Maximise the log-likelihood\n\n\nMaximum Log-Likelihood\nIn Generalised Linear Models (GLMs), we estimate the model's coefficients using a method called maximum likelihood estimation (MLE). The core idea is this: for any given set of candidate parameters, we can compute how likely it is that we would have observed our data, assuming those parameters are true. This is the likelihood.\n\nIf a data point lies close to the predicted curve (or line), it contributes a high likelihood‚Äîit's consistent with the model.\nIf it's far from the predicted values, it contributes a low likelihood‚Äîit‚Äôs inconsistent with the model.\n\n MLE works by searching for the set of parameters (usually denoted as \\(\\beta\\)) that maximise the likelihood function. In other words, we choose the parameters that make our observed data as likely as possible under the model. So, instead of minimising the residual sum of squares like in linear regression, we are maximising the log-likelihood in GLMs. This is particularly useful when dealing with data where assumptions of constant variance or normality don't hold (e.g.¬†count or binary data). A better-fitting model will have a higher log-likelihood, meaning it‚Äôs more consistent with the observed data.\n\n\n\n\n\n\nNote\n\n\n\nEven though two points might look equally close to the predicted line, their likelihood contributions can still differ due to how spread out the underlying distribution is. In Poisson regression, this spread grows with the mean, so deviations are penalized more harshly when the expected value is small.\n\n\n\n\nCode\nset.seed(1)\nx &lt;- 1:20\ny &lt;- rpois(20, lambda = exp(0.2 * x))\nmodel &lt;- glm(y ~ x, family = poisson())\npred &lt;- predict(model, type = \"response\")\nlog_lik &lt;- dpois(y, lambda = pred, log = TRUE)\n\ndf &lt;- data.frame(x, y, pred, log_lik)\n\nggplot(df, aes(x, y)) +\n  geom_point(aes(color = log_lik), size = 5) +\n  geom_line(aes(y = pred), color = \"blue\", linetype = \"dashed\") +\n  scale_color_viridis_c(option = \"virdis\") +\n  labs(title = \"Log-Likelihood Contributions of Points\",\n       subtitle = \"Darker points contribute less to the log-likelihood\",\n       color = \"Log-Likelihood\") +\n  theme_minimal()\n\n\n\n\n\nTo calculate the likelihood function‚Ä¶\n\nCalculate the probability of observing the observations response\nTake the product of these probabilities\n\\(\\ell = \\log(L) = \\sum_{i=1}^{n} \\log \\left[ f(y_i; \\boldsymbol{\\beta}) \\right], \\ where \\ f(y_i; \\boldsymbol{\\beta}) \\ is \\ the \\ pmf \\ of \\ the \\ assumed \\ response \\ distribution\\)\n\n\n\nV. Deviance as a Goodness of Fit\n\n\n\n\n\n\n\nüí° Key Takeaways:\n\n\n\n\nParameter estimation is achieved by minimising the deviance\nThe more variance, the larger the deviance.\nIncreasing model complexity always decreases RSS (LR) / increases MLE (GLM)\nDs ‚â§ D ‚â§ Dn\n1 - pchisq(residuals, df) should be large if our model is correct.\n\n\n\nStatistical analysis does not simply end once we have fit a model. We must determine whether or not our model seems appropriate. ‚ÄúGoodness-of-fit‚Äù is a property that describes how well the data appears to fit the model‚Äôs assumptions. For a GLM, these assumptions are\n\nThe observations are independent.\n\\(g (\\theta) = X \\beta\\) after applying the link function, the parameter of interest is a linear combination of the explanatory terms.\nEach response comes from the assumed distribution.\n\n\nWhen considering a simple GLM, we can add all sorts of explanatory terms;\n\nAdditional numeric explanatory variables\nFactors via dummy variables\nNon linear relationships\nAll of the above\n\n\nBut, adding explanatory terms is sometimes said to make it more ‚Äúcomplex‚Äù, or increase its ‚Äúcomplexity.‚Äù In consequence, increasing a models complexity will almost always\n\nDecrease the residual sum of squares (linear regression), OR\nIncrease the maximised log-likelihood (GLMS).\n\nModel Saturation\n\n\nCode\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(splines)\n\n# Simulate more nonlinear data\nset.seed(42)\nx &lt;- seq(0, 10, length.out = 10)\ny &lt;- 0.5 * x^2 - 3 * x + 5 + rnorm(10, mean = 0, sd = 2)\ndf &lt;- data.frame(x = x, y = y)\n\n# 1. Underfit model: simple linear regression\nmodel_lm &lt;- lm(y ~ x, data = df)\ndf$y_lm &lt;- predict(model_lm)\n\n# 2. Moderately saturated: smooth spline with lower spar (more flexible)\nspline_fit &lt;- smooth.spline(df$x, df$y, spar = 0.4)  # lower spar for more \"wiggle\"\nx_dense &lt;- seq(min(x), max(x), length.out = 200)\nspline_pred &lt;- predict(spline_fit, x_dense)\ndf_spline &lt;- data.frame(x = spline_pred$x, y = spline_pred$y)\n\n# 3. Fully saturated: perfect interpolation\ndf$y_sat &lt;- df$y  # fitted values equal observed values\n\n# Plot 1: Underfit (Linear Regression)\np1 &lt;- ggplot(df, aes(x, y)) +\n  geom_point(color = \"blue\") +\n  geom_line(aes(y = y_lm), color = \"black\") +\n  geom_segment(aes(xend = x, yend = y_lm), color = \"red\", linetype = \"dashed\") +\n  ggtitle(\"Underfit Model\") +\n  theme_minimal()\n\n# Plot 2: Moderately Saturated (Spline Fit)\np2 &lt;- ggplot(df, aes(x, y)) +\n  geom_point(color = \"blue\") +\n  geom_line(data = df_spline, aes(x = x, y = y), color = \"orange\") +\n  geom_segment(aes(xend = x, yend = predict(spline_fit, x)$y), color = \"red\", linetype = \"dashed\") +\n  ggtitle(\"Moderately Saturated Model\") +\n  theme_minimal()\n\n# Plot 3: Fully Saturated (Perfect Fit)\np3 &lt;- ggplot(df, aes(x, y)) +\n  geom_point(color = \"blue\") +\n  geom_line(aes(y = y_sat), color = \"green\") +\n  ggtitle(\"Fully Saturated Model\") +\n  theme_minimal()\n\n# Combine the three plots\ngrid.arrange(p1, p2, p3, nrow = 1)\n\n\n\n\n\nIn almost all cases, the saturated model is not a sensible model as\n\nIf we were to take another sample from the population, their observations would not exhibit the exact same trend.\nThe exact pattern we see therefore reflects the random white noise inherent in our particular sample, and not the underlying relationship that exists in the population.\nWe should try avoid over fitting models, such as the saturated models, so that they only describe the underlying trends in the population, and not random noise from our sample"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "I appreciate you dropping by üòÄ\nMy name‚Äôs Bri. I enjoy statistical modelling and making predictions.\nI graduated with a degree in Statistics from the University of Auckland, have experienced work as a Data Scientist at New Zealands fastest growing start-up, as a Research Analyst at one of Vault‚Äôs top 50 consulting firms, and a Data Analyst at one of top 100 global universities. All in all, I‚Äôm grateful to have such a diverse experience that shapes my knowledge now. I love learning, and I love sharing, so I hope you enjoy the content on this website!\nThis website contains short information bits on statistical modelling distributions, and examples on how to use them. I hope you find them useful, and if nothing else- interesting! I sure did.\n\n\nCode\n# Create a data frame for the smiley face\nsmiley_data &lt;- data.frame(\n  x = c(-0.2, 0.2, -0.1, 0.1, -0.1, 0.1),  # X-coordinates\n  y = c(0.2, 0.2, 0, 0, -0.2, -0.2)       # Y-coordinates\n)\n\n# Create the base plot\np &lt;- ggplot(smiley_data, aes(x, y)) +\n  xlim(-1, 1) + ylim(-1, 1) +  # Set the plot limits\n  theme_void()                  # Remove background and axis\n\n# Add the face\np &lt;- p +\n \n  geom_point(aes(x = -0.3, y = 0.4), size = 10, color = \"black\") +\n  geom_point(aes(x = 0.3, y = 0.4), size = 10, color = \"black\")\n\n# Add the nose\np &lt;- p +\n  geom_point(aes(x = 0, y = 0.), size = 10, color = \"red\")\n\n# Add the smile\np &lt;- p +\n  geom_curve(\n    aes(x = -0.4, y = -0.6, xend = 0.4, yend = -0.6),\n    color = \"black\", size = 2, curvature = 0.4\n  )\n\n \n\n# Display the plot\nprint(p)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "poisson_earthquakes.html",
    "href": "poisson_earthquakes.html",
    "title": "Poisson Modelling",
    "section": "",
    "text": "Introduction\n\nWhen is the next volcano due to erupt? Any moment now, unfortunately! (Give or take 1000 years or so).\nA volcano could happen this afternoon, or it might not happen for another 1000 years. Volcanoes are almost impossible to predict; they seem to happen completely at random.\nHowever, we do know of a statistical distribution that counts the number of events in a fixed space of time‚Ä¶the Poisson distribution! The Poisson process counts the number of events occurring in a fixed time or space, when events occur independently and at a constant average rate, where the formula for our statistical model is as follows:\n\\(Y_i \\sim \\text{Poisson}(\\mu_i),\\) where \\(\\mu_i = \\beta_0 + \\beta_1 x_i\\quad\\)\nDiving a bit into understanding the foundations of how we will use this statistical model in R‚Ä¶ It is worth noting that we need to use a link function to map our parameter of interest, \\(\\mu_i\\) , to the real number line, where \\(log(\\mu_i) = \\beta_0 + \\beta_1 x_i\\).\nIn other words, the expected value of our response variable, \\(\\mu_i\\), must be non negative \\((\\mu_i\\ ‚â• 0)\\). However, because we are using a log-link function, the model actually estimates \\(\\log(\\mu_i)\\) which can take on any real number, that is \\(- \\infty &lt; \\log(\\mu_i) &lt; +\\infty\\)\nToday, we will look at how to use the poisson distribution to model the occurrence of volcanic events per year in Country A. This country is composed of smaller areas that regularly record information on the following:\n\narea.type: Categorical variable with 9 categories.\ntotal.land.area: Total land area in square kms\ntotal.area: Total area (including water) in square kms\nshape.length: Length of the shape of the area in kms\nUrbanRural: Recoded variable area.type into bigger categories\nprevious.eruption: Number of previous volcanic eruptions in the are\nY: Number of eruptions during the observation period. The observation time is ONE\nyear for all areas.\nearthquakes.year: Average number of earthquakes per year in the area\n\nA quick note, this blog is not focused on the exploratory part of this data, and will not go in-depth to all the metrics and terms used, but I aim to add another page soon explaining the terminology!\n\n\nStep 1. Choosing variables to include in the model\n\nEach analysis begins with selecting which variables to include in our model. Contrary to intuition, we do not want to include every variable available ‚Äî for several key reasons:\n\nOccam‚Äôs Razor suggests that, all else equal, simpler models are preferable to more complex ones.\nIncluding too many variables can introduce multicollinearity, where highly correlated predictors make coefficient estimates unstable and unreliable.\n\nThe methods to identify appropriate explanatory variables are listed below (this is specific to our analysis):\n\nDomain knowledge: Always the most important source of insight. Correlation plots: Useful for detecting multicollinearity between explanatory variables.\nChi-square tests: To test for associations between categorical variables.\nAkaike Information Criterion (AIC): Balances model fit with model complexity to guide selection relative to comparative models.\n\nWe aim to choose explanatory variables that are strongly associated with the response variable while avoiding those that are highly correlated with each other. We will walk through choosing appropriate variables to model our response, Y, with an example, focusing on the variables area.type and UrbanRural\nIn the pairs plot below, we can see area.type and UrbanRuralare moderately colinear with a correlation of 0.42 and therefore only one should in the model.\n\n\nCode\npairs20x(data)\n\n\n\n\n\n\n1.1 Domain Knowledge\nIt‚Äôs important to ask whether there is a contextual benefit to including area.type or UrbanRural. As we are interested in modelling the frequency of eruptions, and not the impact of specific area types, I am unbiased as to which variable we include in the model from a contextual standpoint.\n\n\n1.2 Chi-square Tests\nAs established by our pairs plot, it looks like we face a few colinear variables, and we are interested in looking into UrbanRural and area.type.\nWe can test the hypothesis that they both variables contain similar information using a chi-square test. This test checks for a statistically significant association between two categorical variables, with the null hypothesis being that there is there is no association between two categorical variables.\n\\[\nH_0: \\text{The two categorical variables are independent.}\n\\]\nBelow, we can see the test result has a small p-value of 0.0005, thus showing no evidence to reject the null hypothesis ‚Äî suggesting that the distribution of UrbanRural and area.type categories is likely due to chance, and they are not significantly different.\nGiven area.type and UrbanRural are not statistically different, and they have eight anad three levels, respectively, we will opt for the simpler explanatory variable with three levels, UrbanRural.\nWhy do we want to reduce our model complexity? Well, simpler models are easier to interpret when we are quantifying their impact on the response. Remember, this is statistical modelling NOT machine learning. We need to be able to explain every part going on, and the effect each variable has on the result.\nFrom a computational standpoint, it enables a faster convergence of models, allows parameter coefficients to be more stable, and helps avoids overfitting.\n\n\nCode\n # We can see that we have a small p-value of 0.00049 (p-val &lt; 0.05), therefore have no evidence to reject the null hypothesis that the observed distribution difference across the categories 'UrbanRural` and `area.type` is due to chance \ntabledata &lt;- table(data$UrbanRural, data$area.type)\nchiResult &lt;- chisq.test(tabledata, simulate.p.value = TRUE, B = 2000)\n\n# Clean summary\ncat(\"Chi-squared test (with simulated p-value, B = 2000):\\n\")\n\n\nChi-squared test (with simulated p-value, B = 2000):\n\n\nCode\ncat(\"X-squared =\", round(chiResult$statistic, 3), \"\\n\")\n\n\nX-squared = 37478 \n\n\nCode\ncat(\"Degrees of freedom =\", chiResult$parameter, \"\\n\")\n\n\nDegrees of freedom = NA \n\n\nCode\ncat(\"P-value =\", format.pval(chiResult$p.value, digits = 4, eps = 1e-4), \"\\n\")\n\n\nP-value = 0.0004998 \n\n\n\n\n1.3 AIC\nAIC is a likelihood based metric often used to compare two statistical models. A lower AIC is favourable to a higher AIC, as it indicates a better blend of model complexity while still explaining the data well.\nIt‚Äôs important to note that there is no single ‚Äúcorrect‚Äù way to choose variables. For instance, UrbanRural has a slightly higher AIC (253157) than area.type (253043), but the difference is marginal and in my personal opinion, not worth incorporating into the model due to its additional levels. (UrbanRural has 3 levels while area.type has 5).\n\n\nCode\nmodel1 &lt;- glm(Y ~ UrbanRural, family = \"poisson\", data = data)\nmodel2 &lt;- glm(Y ~ area.type, family = \"poisson\", data = data)\n\n# Create comparison data frame\ncomparison &lt;- data.frame(\n  Model = c(\"UrbanRural\", \"Area Type\"),\n  AIC = c(AIC(model1), AIC(model2)))\n\n# Pretty table\ncomparison %&gt;%\n  kable(\"html\", caption = \"&lt;strong&gt; Model Comparison: AIC &lt;/strong&gt;\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), \n                full_width = FALSE, \n                position = \"center\")\n\n\n\n\nModel Comparison: AIC\n\n\nModel\nAIC\n\n\n\n\nUrbanRural\n253157.0\n\n\nArea Type\n253042.8\n\n\n\n\n\n\n\n\nLastly, our plots indicate both variables follow similar distribution patterns. The effect of Sea, which only has three observations, is negligible and including it would risk biasing our model due to its higher proportion of earthquakes, but smaller sample size.\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(patchwork)\n\n# Ensure categorical variables are factors\ndata &lt;- data %&gt;%\n  mutate(\n    UrbanRural = factor(UrbanRural),\n    area.type = factor(area.type)\n  )\n\n# --- Add counts directly to data frame ---\ndata_urban &lt;- data %&gt;%\n  group_by(UrbanRural) %&gt;%\n  mutate(n = n()) %&gt;%\n  ungroup()\n\ndata_area &lt;- data %&gt;%\n  group_by(area.type) %&gt;%\n  mutate(n = n()) %&gt;%\n  ungroup()\n\n# --- Plot for UrbanRural ---\np1 &lt;- ggplot(data_urban, aes(x = UrbanRural, y = Y)) +\n  geom_boxplot(fill = \"lightpink\") +\n  stat_summary(\n    fun = max, geom = \"text\",\n    aes(label = paste0(\"n = \", n)),\n    vjust = -0.5, size = 3\n  ) +\n  labs(\n    title = \"Earthquakes per Urban/Rural Area\",\n    x = \"UrbanRural\",\n    y = \"Earthquakes per Year\"\n  ) +\n  theme_minimal()\n\n# --- Plot for area.type ---\np2 &lt;- ggplot(data_area, aes(x = area.type, y = Y)) +\n  geom_boxplot(fill = \"lightblue\") +\n  stat_summary(\n    fun = max, geom = \"text\",\n    aes(label = paste0(\"n = \", n)),\n    vjust = -0.5, size = 3\n  ) +\n  labs(\n    title = \"Earthquakes per Area Type\",\n    x = \"Area Type\",\n    y = \"Earthquakes per Year\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# --- Combine plots side by side ---\np1\n\n\n\n\n\nCode\np2\n\n\n\n\n\n\n\n\nStep 2: Fitting the model\n\nGiven that the response variable, Y, represents a count of discrete positive numbers, we have chosen to apply a generalized linear model that utilities a Poisson distribution to predict Y, the frequency of eruptions.\nTo convert the expected average count of eruptions, denoted as ¬µi, from the Poisson distribution into a linear predictor, we have used the log link function. This is because we must ensure all expected values are greater than 0 (our model should not output an expected negative amount of earthquakes!), account for non constant variance, and assume a discrete responses (we can‚Äôt have 2.5 earthquakes either! what would half an earthquake mean?)\nWe have used explanatory variables; previous.eruption, UrbanRural, earthquakes.year, and total.area. All explanatory variables had significant contributions towards explaining model variance (pval : &lt;2e-16).\nWe will initially fit a model with all the variables and see which is the optimal outcome based on which model has the lowest AIC, thereby penalizing model complexity and rewarding goodness of fit. In addition, We iteratively removed insignificant variables each model to final converge on glm_model4, which had the (subjectively) best combination of low AIC and low model complexity.\nHowever, not all assumptions were not satisfied, as we can see an increase in variance with an increase in the Y (number of eruptions) as the mean increases. If our model was perfect, our predicted values would follow closely to the purple line.\n\n\nCode\n############################################################\n# Split data into test and training\n############################################################\ntrain_index &lt;- createDataPartition(data$Y, p = 0.8, list = FALSE)\ntrain_data &lt;- data[train_index, ]\ntest_data &lt;- data[-train_index, ]\n\n\n#Fit model with all predictor variables and see what the model tries best with stepwise regression \nglm_model1 &lt;- glm(Y ~ previous.eruption + UrbanRural + shape.length + total.area + total.land.area + earthquakes.year, family = \"poisson\", data = train_data)\n#summary(glm_model1)\n\nglm_model2 &lt;- glm(Y ~ previous.eruption + earthquakes.year + UrbanRural + total.area + shape.length, data = train_data, family = poisson)\n# summary(glm_model2)\n\nglm_model3 &lt;- glm(Y ~ UrbanRural + previous.eruption + earthquakes.year, data = train_data, family = poisson)\n# summary(glm_model3)\n\n# Best model \nglm_model4 &lt;- glm(Y ~ previous.eruption + earthquakes.year + UrbanRural + total.area, family = poisson, data = train_data)\n#summary(glm_model4)\n\n\n# MODEL COMPARISON \n#glm_model3 has the best median between low AIC and low model complexity \nAIC(glm_model1, glm_model2, glm_model3, glm_model4)\n\n\n           df      AIC\nglm_model1  8 146322.3\nglm_model2  7 146326.6\nglm_model3  5 146907.7\nglm_model4  6 146387.1\n\n\nCode\npredictions_glm4 &lt;- predict(glm_model4, newdata = test_data, type = \"response\")\ntest_data$predictions_glm4 &lt;- predictions_glm4\n\n# kind of bad model \nggplot(test_data, aes(x = Y, y = predictions_glm4, col = UrbanRural)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"loess\", se = FALSE) +\n  geom_line(aes(x = Y, y = Y), color = \"purple\", alpha = 0.5) + # Ideal fit line\n  labs(x = \"Actual\", y = \"Predicted\", title = \"Actual vs. Predicted values from chosen Poisson model \") + \n  theme_minimal()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCode\n##############################\n# Fit model with full data now \n##############################\nglm_model3_full &lt;- glm(Y ~ UrbanRural + previous.eruption + earthquakes.year + total.area,\n                 family = \"poisson\", data = data)\n\n# summary(glm_model3_full)\n\n\n\n\nStep 3: Diagnostic checks on our chosen model\n\nBelow, we run diagnostic checks on our chosen model\n\nThe Chi-squared ANOVA checks if all levels are significant within our model. For example, the UrbanRural variable has 3 levels (degrees of freedom +1) , and we can see from the significant P-value of &lt;2.2e-16 that they are all significant.\n\n\nCode\n# All levels are significant for our best model\nanova(glm_model4, test = \"Chisq\")\n\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: Y\n\nTerms added sequentially (first to last)\n\n                  Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \nNULL                              14993     160652              \nprevious.eruption  1    55873     14992     104778 &lt; 2.2e-16 ***\nearthquakes.year   1      113     14991     104665 &lt; 2.2e-16 ***\nUrbanRural         2    11094     14989      93571 &lt; 2.2e-16 ***\ntotal.area         1      523     14988      93048 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCooks distance measures the influence of a specific data point on the overall model. For example, it quantifies how much the models predictions would change if these datapoints were removed. Below, we can see there are a few datapoints that clearly stick out, but all have a distance under 0.5 so we are not concerned about their influence.\n\n\nCode\n# A simple function to generate a cooks distance plot \ncooks20x(glm_model3)\n\n\n\n\n\nWe use a normality check to see how the residuals of our model are distributed. Residuals are the differences between the observed values and the values predicted by out model, simply put as\n\n\\[\nr_i = y_i - \\hat{y}_i$.\n\\]\n\n\nCode\nnormcheck(glm_model3)\n\n\n\n\n\n\n\nStep 4: Quantifying the impact of our variables on the frequency of earthquakes\n\nWe have evidence to reject the null hypothesis that the model is a good fit (pval : 0) .\nAll variables were significant in explaining the variance in our response (p-val &lt; 0.05).\nHowever, we can visually see that this model still offers some useful information from the above plot.\nHolding all other variables constant:\n\nWe estimate that, for every 1 square km increase in total area, the expected number of eruptions for a given time period increases by 1.86%\nWe estimate that, for every 1 unit increase in previous eruptions, the expected number of eruptions for a given time period decrease by 76.61%\nWe estimate that, for every 1 unit increase in earthquakes.year (average number of earthquakes per year in the area), the expected number of eruptions for a given time period increase by 4.13%.\nWe estimate that the expected number of earthquakes in a given time period for the ‚ÄúSettlement‚Äù Urban Rural category is 26.3% higher than the expected number of earthquakes in the ‚ÄúOther‚Äù category.\nWe estimate that the expected number of earthquakes in a given time period for the ‚ÄúUrban‚Äù Urban Rural category is 44.84% lower than the expected number of earthquakes in the ‚ÄúOther‚Äù category.\n\n\n\nCode\nsummary(glm_model3_full) \n\n\n\nCall:\nglm(formula = Y ~ UrbanRural + previous.eruption + earthquakes.year + \n    total.area, family = \"poisson\", data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-6.9865  -2.3027  -0.7174   1.2060  10.2700  \n\nCoefficients:\n                             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 3.1674850  0.0079876  396.55   &lt;2e-16 ***\nUrbanRuralRural settlement  0.2337399  0.0131489   17.78   &lt;2e-16 ***\nUrbanRuralUrban            -0.5950103  0.0078943  -75.37   &lt;2e-16 ***\nprevious.eruption          -1.4528968  0.0070815 -205.17   &lt;2e-16 ***\nearthquakes.year            0.0404475  0.0027978   14.46   &lt;2e-16 ***\ntotal.area                  0.0184193  0.0006451   28.55   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 200101  on 18738  degrees of freedom\nResidual deviance: 116577  on 18733  degrees of freedom\nAIC: 183123\n\nNumber of Fisher Scoring iterations: 5\n\n\nCode\n100*(exp(coef(glm_model3_full))-1)  \n\n\n               (Intercept) UrbanRuralRural settlement \n               2274.768259                  26.331584 \n           UrbanRuralUrban          previous.eruption \n                -44.844311                 -76.610823 \n          earthquakes.year                 total.area \n                  4.127662                   1.858996 \n\n\n\n\nStep 5: Adding more complexity to our model to improve its explanatory power\n\nGreat! Now that we have established our baseline model, it‚Äôs time to see how we can improve it one way to do this is by adding interaction effects. Interaction effects take into account the effect of one explanatory variable on the response variable, depending on the level of another explanatory variable. For example, our UrbanRural variable has 3 levels, and the effect of each level on the response may differ based on the other explanatory variables.\nBelow, we explore one interaction effect, and notice a distinction between previous.eruption and earthquakes.year. The interaction was fit and kept due it‚Äôs significant p-value (pval : &lt; 2.2e-16).\n\n\nCode\n##############################\n# Lets investigate the relationships\n##############################\nfit1 &lt;- glm(Y ~ previous.eruption*earthquakes.year, data = data, family = poisson)\ninteract_plot(fit1, pred = previous.eruption, modx = earthquakes.year)\n\n\n\n\n\nAfter comparing our previous simple poisson model, with our current poisson model with an interaction effect, we can see the interaction model actually provides a lower AIC!\n\n\nCode\n##############################\n# Fit model with full data now \n##############################\nglm_model5 &lt;- glm(Y ~ previous.eruption*earthquakes.year + UrbanRural + total.area, family = poisson, data = data)\nsummary(glm_model5) # All significant \n\n\n\nCall:\nglm(formula = Y ~ previous.eruption * earthquakes.year + UrbanRural + \n    total.area, family = poisson, data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-6.9849  -2.0430  -0.5682   1.2069   9.8643  \n\nCoefficients:\n                                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         3.121215   0.008044  388.01   &lt;2e-16 ***\nprevious.eruption                  -1.002563   0.009295 -107.87   &lt;2e-16 ***\nearthquakes.year                    0.093767   0.002908   32.24   &lt;2e-16 ***\nUrbanRuralRural settlement          0.233251   0.013147   17.74   &lt;2e-16 ***\nUrbanRuralUrban                    -0.597354   0.007891  -75.70   &lt;2e-16 ***\ntotal.area                          0.018134   0.000645   28.12   &lt;2e-16 ***\nprevious.eruption:earthquakes.year -0.599499   0.010139  -59.12   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 200101  on 18738  degrees of freedom\nResidual deviance: 112713  on 18732  degrees of freedom\nAIC: 179261\n\nNumber of Fisher Scoring iterations: 5\n\n\nCode\nAIC(glm_model4, glm_model5)\n\n\nWarning in AIC.default(glm_model4, glm_model5): models are not all fitted to\nthe same number of observations\n\n\n           df      AIC\nglm_model4  6 146387.1\nglm_model5  7 179261.2\n\n\nWe can also see the predictions from this model perform slightly better than our previous model.\n\n\nCode\n##############################\n# SHOW PREDICTIONS\n##############################\npredictions_glm5 &lt;- predict(glm_model5, newdata = test_data, type = \"response\")\ntest_data$predictions_glm5 &lt;- predictions_glm5\n\n# kind of bad model \nggplot(test_data, aes(x = Y, y = predictions_glm5, col = UrbanRural)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"loess\", se = FALSE) +\n  geom_line(aes(x = Y, y = Y), color = \"red\", alpha = 0.5) + # Ideal fit line\n  labs(x = \"Actual\", y = \"Predicted\", title = \"Actual vs. Predicted Poisson model \") + \n  theme_minimal()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nSimilar to step Step 2, we must run diagnostic checks on our model. After all, we can‚Äôt blindly code something and believe it to be accurate!\nWe can see our interaction is significant with a p-value of &lt;2.2e-17\n\n\nCode\nanova(glm_model5, test = \"Chisq\")\n\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: Y\n\nTerms added sequentially (first to last)\n\n                                   Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)\nNULL                                               18738     200101          \nprevious.eruption                   1    69679     18737     130423 &lt; 2.2e-16\nearthquakes.year                    1      184     18736     130238 &lt; 2.2e-16\nUrbanRural                          2    12950     18734     117288 &lt; 2.2e-16\ntotal.area                          1      711     18733     116577 &lt; 2.2e-16\nprevious.eruption:earthquakes.year  1     3864     18732     112713 &lt; 2.2e-16\n                                      \nNULL                                  \nprevious.eruption                  ***\nearthquakes.year                   ***\nUrbanRural                         ***\ntotal.area                         ***\nprevious.eruption:earthquakes.year ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# ##############################\n# # Diagnostic checks\n# ##############################\n# cooks20x(glm_model5)\n# plot(glm_model5, lty = 2, pch = substr(data$UrbanRural, 1, 1))\n\n\n\n\nStep 6: Concluding statements and interpretations from our final model\n\nAs the response variable, number of eruptions during the observation period, is a count, we have fit a generalized linear model with a Poisson response distribution.\nWe have 5 significant explanatory terms (pval all &lt;0.05); earthquakes.year, total.area, previous.eruption, total.area, and UrbanRural. The interaction plot above suggests a difference between the expected number of eruptions depending on how many eruptions there were in the previous year (per area) and the average number of eruptions (per area).\nHence, a poisson model with an interaction term between previous.eruption and earthquakes.year was fit. The interaction term was significant (pval : &lt; 2e-16) so it remained in the model.\nAll assumptions were satisfied. However, we still cannot trust the results from the Poisson model (pval: 0 )\nHolding all other variables constant‚Ä¶\n\nWe estimate that, for every 1 unit increase in the number of previous volcanic eruptions (previous.eruptions), the expected number eruptions in a given time period decreases by 63.3%.\n\nThis is further decreased by 45.1% for every increase in the average number of earthquakes per year in the area (earthquakes.year)\n\nWe estimate that, for every 1 unit increase in the number of average earthquakes per year, the expected number of eruptions in a given time period increases by 9.83%.\n\nThere is further decreased by 45.1% for every increase the number of previous volcanic eruptions (previous.eruption).\n\nWe estimate that, for every 1 square km increase in total area, the expected number of eruptions in a given time period increases by 1.83%\nWe estimate that the expected number of eruptions in a given time period is 44.97% lower in ‚ÄúUrban‚Äù (UrbanRural) areas compared to ‚ÄúOther‚Äù areas\nWe estimate that the expected number of eruptions in a given period is 26.27% higher in ‚ÄúSettlement‚Äù (UrbanRural) areas compared to ‚ÄúOther areas‚Äù\n\n\n\nCode\nsummary(glm_model5)\n\n\n\nCall:\nglm(formula = Y ~ previous.eruption * earthquakes.year + UrbanRural + \n    total.area, family = poisson, data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-6.9849  -2.0430  -0.5682   1.2069   9.8643  \n\nCoefficients:\n                                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         3.121215   0.008044  388.01   &lt;2e-16 ***\nprevious.eruption                  -1.002563   0.009295 -107.87   &lt;2e-16 ***\nearthquakes.year                    0.093767   0.002908   32.24   &lt;2e-16 ***\nUrbanRuralRural settlement          0.233251   0.013147   17.74   &lt;2e-16 ***\nUrbanRuralUrban                    -0.597354   0.007891  -75.70   &lt;2e-16 ***\ntotal.area                          0.018134   0.000645   28.12   &lt;2e-16 ***\nprevious.eruption:earthquakes.year -0.599499   0.010139  -59.12   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 200101  on 18738  degrees of freedom\nResidual deviance: 112713  on 18732  degrees of freedom\nAIC: 179261\n\nNumber of Fisher Scoring iterations: 5\n\n\nCode\n# pval_glm_model5 &lt;- 1 - pchisq(90402, 14987)\n# print(paste(\"We cannot still trust the most optimal model found as it has a small p-value of\", pval_glm_model5, \"indicating the poisson model does not provide a good fit \"))\n\n#Interpreting coefficients\n100*(exp(coef(glm_model5))-1)  \n\n\n                       (Intercept)                  previous.eruption \n                       2167.392233                         -63.306220 \n                  earthquakes.year         UrbanRuralRural settlement \n                          9.830385                          26.269789 \n                   UrbanRuralUrban                         total.area \n                        -44.973416                           1.829907 \nprevious.eruption:earthquakes.year \n                        -45.091323 \n\n\n\n\nExtra : Mathematical interpretation of our model\n\nOur final model is:\n\\[\nlog(\\mu_i)=\\beta_0+\\beta_1p_i+\\beta_2e_i+\\beta_3s_i+\\beta_4u_i+\\beta_5t_i+\\beta_6p_ie_i\n\\]\nWhere :\n\n\\(\\mu_i\\) is the number of eruptions during the observation period and Y (the expected number of eruptions in the ‚ÄúOther‚Äù Urban Area) has a poisson distribution with mean \\(\\mu_i\\)\n\\(\\text{p}_{i}\\) represents the number of previous eruptions in the area\n\\(\\text{e}_{i}\\) represents the average number of earthquakes per year in the area\nThe factor variable \\(\\text{UrbanRural}\\) has 3 dummy variables where \\(\\text{s}_{i}\\) takes the value 1 if the observation is from Settlement and 0 otherwise. In addition \\(\\text{u}_{i}\\) takes the value 1 if the observation is from Urban, and 0 otherwise.\n\\(\\text{t}_{i}\\) represents the total area (including water) in square km.\n\nHere, ‚ÄúOther‚Äù (within the UrbanRural factor) is our baseline level."
  },
  {
    "objectID": "binomial_image.html",
    "href": "binomial_image.html",
    "title": "Binomial Image Recognition",
    "section": "",
    "text": "Introduction\nThe US post office want to electronically scan hand-written numbers and be able to predict what was written for the ZIP code. A ZIP code tells them where a mail item needs to be delivered. We will look at the numbers 3 and 7 only ‚Äî to see if we can distinguish these two numbers ‚Äî to illustrate ideas about prediction based on logistic regression.\nIn this dataset, each hand-written digit is represented as a 16 x 16 array of pixels.\nEach pixel is given a grey-scale value in the range [‚àí1, 1] with -1 representing white and 1 representing black. There are thus 16 x 16 = 256 numbers representing a particular digit, which we can take as the values of 256 variables, v1, . . . , v256, say\n\n\nStep 1: Reading in data\n\n\nCode\n# Note: -1 is white and 1 is black \n\ntrain.df &lt;- read.table(\"data/binomial_image/train.txt\")\ntest.df &lt;- read.table(\"data/binomial_image/test.txt\")\n\n# Append D to the dataframe\nnames(train.df) = c(\"D\", paste(\"V\", 1:256, sep=\"\"))\nnames(test.df) = c(\"D\", paste(\"V\", 1:256, sep = \"\"))\n\n\n\n\nStep 2: View the first 25 samples of handwritten 3s and 7s to identify which are the best at discriminating\nHere, we can see cells that would be best at differentiating between a 3 and 7 are the cells that have most structural difference of white and black space i.e.¬†protruding instances of black colour that are in areas unique to that number.\nThis can be seen in the following areas of each image:\n\nBottom Cells: It has a very pronounced black curve in the bottom right corner, which forms a loop. This is very contrasting to a 7, which remains open and essentially finishes being black at the bottom right.\nMiddle: The middle of this three is black and extends far from the right to the left of the cell, (almost like a straight line) providing more pronounced differentiation compared to a 7, which does not have this feature in this data set and instead has white area.\nTop-left: The top of this cell is curved and filled in black, while a 7 in this data set typically has a distinct diagonal black line in the top-left corner.\n\n\n\nCode\n# Set up the plotting area to have a 5x5 grid and minimal margins\npar(mfrow=c(5,5), mar = c(1,1,1,1))\n# Loop through the first 25 rows of the data frame\nfor(k in 1:25){\n  \n  # Convert the k-th row (excluding the first column) into a 16x16 matrix\n  z = matrix(unlist(train.df[k,-1]), 16,16)\n  \n  # Initialize zz as a copy of z\n  zz = z\n  \n    # Mirror the matrix horizontally\n  for(j in 16:1)zz[,j]=z[,17-j]\n  \n  # Plot the mirrored matrix as an image with grayscale colors\n  image(zz, col = gray((32:0)/32))\n  \n  # Add a box around the plot\n  box()\n\n  # Add the value of D in the top-left corner of the plot\n  text(0.1,0.9,train.df$D[k], cex=1.5)\n}\n\n\n\n\n\n\n\nStep 3: Computing the correlation\nHere, we will compute the correlation between D and V1, V2,‚Ä¶ V256, and identify which of these variables have the highest absolute correlations (i.e either very large and negative or very large and positive).\n\n\nCode\n# Correlation with D, without D\ncrs=cor(train.df)[,\"D\"][-1]\n\n# Sort absolute values in in decending order\nsort(abs(crs), decreasing=T)[1:20]\n\n\n     V185      V170      V105      V220      V235      V201      V229      V120 \n0.8714422 0.8331768 0.8208550 0.8144829 0.8053422 0.7994677 0.7824515 0.7795704 \n     V219      V230      V104      V189      V205      V169      V234      V121 \n0.7767427 0.7646643 0.7578159 0.7554507 0.7546277 0.7504911 0.7430652 0.7393237 \n     V204      V186      V173      V221 \n0.7375292 0.7332635 0.7254096 0.6929391"
  },
  {
    "objectID": "binomial_introduction.html",
    "href": "binomial_introduction.html",
    "title": "Introduction to Binomial Modelling",
    "section": "",
    "text": "Differences in data distributions and how you should model,where the distribution is a function that shows the possible values of a variable and how frequently they occur.\n\nDistributions\n\n\nCode\n# ------------------------------\n# Normal Distribution (mean = 0, sd = 1)\n# ------------------------------\nR &lt;- 1000000\ny_norm &lt;- rnorm(R, mean = 0, sd = 1)\nhist(y_norm, prob = TRUE, \n     main = \"Histogram of Normal Distribution\", \n     xlab = \"Value\", col = \"lightgray\", border = \"white\")\n\nlines(density(y_norm), col = \"red\", lwd = 2)\n\n\n\n\n\nCode\n# ------------------------------\n# Binomial Distribution (n = 20, p = 0.3)\n# ------------------------------\nn &lt;- 20\np &lt;- 0.3\n\ny_binom &lt;- rbinom(R, size = n, prob = p)\n\nhist(y_binom, prob = TRUE, breaks = n,\n     main = \"Histogram of Binomial Distribution\",\n     xlab = \"Number of Successes\", col = \"lightblue\", border = \"white\")\n\nlines(density(y_binom), col = \"red\", lwd = 2)\n\n\n\n\n\nCode\n# ------------------------------\n# Poisson Distribution (lambda = 3)\n# ------------------------------\nlambda &lt;- 3\ny_pois &lt;- rpois(R, lambda = lambda)\n\nhist(y_pois, prob = TRUE, breaks = max(y_pois),\n     main = \"Histogram of Poisson Distribution\",\n     xlab = \"Count\", col = \"lightgreen\", border = \"white\")\n\nlines(density(y_pois), col = \"red\", lwd = 2)"
  }
]